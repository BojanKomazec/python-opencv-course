{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Matching - Part 1\n",
    "\n",
    "* Template Matching that we did before finds the part of the bigger image which exactly matches the smaller image. So smaller image is fully contained in the bigger image. We know it's there. We have its exact copy. E.g. we had exact copy image of the dog's face.\n",
    "\n",
    "\n",
    "* Feature Matching:\n",
    "   * extracts defining key features (corners, edges, contours) from an input/target image (an image we'll try to find in the bigger image)\n",
    "   * using a distance calculation finds all the matches in another image; what's being matched are features but not 1:1 pixels like in Template Matching\n",
    "   * no exact copy of the target image is required; input image can be some other dog's face image where, taken from a different angle etc...\n",
    "   \n",
    "\n",
    "* Feature Matching methods we'll be using here are:\n",
    "   * Brute-Force Matching with ORB Descriptors\n",
    "   * Brute-Force Matching with SIFT Descriptors and Ratio Test\n",
    "   * FLANN-based Matcher\n",
    "   \n",
    "   \n",
    "[Feature Matching](https://docs.opencv.org/master/dc/dc3/tutorial_py_matcher.html)\n",
    "   \n",
    "[Feature Detection and Description](https://docs.opencv.org/master/db/d27/tutorial_py_table_of_contents_feature2d.html)\n",
    "\n",
    "**Keypoints** are the same thing as interest points. They are spatial locations, or points in the image that define what is interesting or what stand out in the image.\n",
    "\n",
    "From [Meaning of Keypoints and Descriptors](https://answers.opencv.org/question/37985/meaning-of-keypoints-and-descriptors/):\n",
    "\n",
    "**Descriptors**: they are the way to compare the keypoints. They summarize, in vector format (of constant length) some characteristics about the keypoints. For example, it could be their intensity in the direction of their most pronounced orientation. It's assigning a numerical description to the area of the image the keypoint refers to.\n",
    "\n",
    "Some important things for descriptors are:\n",
    "   * they should be independent of keypoint position. If the same keypoint is extracted at different positions (e.g. because of translation) the descriptor should be the same.\n",
    "   * they should be robust against image transformations. Some examples are changes of contrast (e.g. image of the same place during a sunny and cloudy day) and changes of perspective (image of a building from center-right and center-left, we would still like to recognize it as a same building). Of course, no descriptor is completely robust against all transformations (nor against any single one if it is strong, e.g. big change in perspective). Different descriptors are designed to be robust against different transformations which is sometimes opposed to the speed it takes to calculate them.\n",
    "   * they should be scale independent. The descriptors should take scale in to account. If the \"prominent\" part of the one keypoint is a vertical line of 10px (inside a circular area with radius of 8px), and the prominent part of another a vertical line of 5px (inside a circular area with radius of 4px) -- these keypoints should be assigned similar descriptors.\n",
    "\n",
    "Now, that you calculated descriptors for all the keypoinst, you have a way to compare those keypoints. For a simple example of image matching (when you know the images are of the same object, and would like to identify the parts in different images that depict the same part of the scene, or would like to identify the perspective change between two images), you would compare every keypoint descriptor of one image to every keypoint descriptor of the other image. As the descriptors are vectors of numbers, you can compare them with something as simple as Euclidian distance. There are some more complex distances that can be used as a similarity measure, of course. But, in the end, you would say that the keypoints whose descriptors have the smallest distance between them are matches, e.g. same \"places\" or \"parts of objects\" in different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(img, cmap='gray'):\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reeses = cv2.imread('../data/reeses_puffs.png', 0)\n",
    "display(reeses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cereals = cv2.imread('../data/many_cereals.jpg', 0)\n",
    "display(cereals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-Force Matching with ORB Descriptors\n",
    "\n",
    "Brute-Force matcher takes the descriptor of one feature in first set and is matched with all other features in second set using some distance calculation. And the closest one is returned.\n",
    "\n",
    "[ORB (Oriented FAST and Rotated BRIEF) (2011)](https://docs.opencv.org/3.4/d1/d89/tutorial_py_orb.html)\n",
    "\n",
    "[ORB: an efficient alternative to SIFT or SURF](https://www.gwylab.com/download/ORB_2012.pdf)\n",
    " \n",
    " \n",
    "* ORB (Oriented FAST and Rotated BRIEF) is a keypoint detector and descriptor extractor\n",
    "* computationally-efficient replacement to SIFT that has similar matching performance, is less affected by image noise, and is capable of being used for real-time performance.\n",
    "* a fusion of FAST keypoint detector and BRIEF descriptor with many modifications to enhance the performance\n",
    "\n",
    "[cv2::Feature2D Class Reference](https://docs.opencv.org/3.4/d0/d13/classcv_1_1Feature2D.html#a8be0d1c20b08eb867184b8d74c15a677)\n",
    "* Abstract base class for 2D image feature detectors and descriptor extractors.\n",
    "\n",
    "[cv2::ORB Class Reference](https://docs.opencv.org/3.4/db/d95/classcv_1_1ORB.html)\n",
    "* Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor.\n",
    "* `cv2::ORB` implements `cv2::Feature2D`\n",
    "\n",
    "[retval = cv2.ORB_create(...)](https://docs.opencv.org/3.4/db/d95/classcv_1_1ORB.html#adc371099dc902a9674bd98936e79739c)\n",
    "* The ORB constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb = cv2.ORB_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[keypoints, descriptors = cv.Feature2D.detectAndCompute(image, mask\\[, descriptors\\[, useProvidedKeypoints\\]\\])](https://docs.opencv.org/3.4/d0/d13/classcv_1_1Feature2D.html#a8be0d1c20b08eb867184b8d74c15a677)\n",
    "* Detects keypoints and computes the descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyPoints1, descriptors1 = orb.detectAndCompute(reeses, None)\n",
    "keyPoints2, descriptors2 = orb.detectAndCompute(cereals, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'type(keyPoints1) = {type(keyPoints1)}')\n",
    "print(f'len(keyPoints1) = {len(keyPoints1)}')\n",
    "print(f'type(keyPoints1[0]) = {type(keyPoints1[0])}')\n",
    "print(f'keyPoints1[0] = {keyPoints1[0]}')\n",
    "\n",
    "print(f'type(descriptors1) = {type(descriptors1)}')\n",
    "print(f'len(descriptors1) = {len(descriptors1)}')\n",
    "print(f'type(descriptors1[0]) = {type(descriptors1[0])}')\n",
    "print(f'descriptors1[0] = {descriptors1[0]}')\n",
    "\n",
    "print(f'type(keyPoints2) = {type(keyPoints2)}')\n",
    "print(f'len(keyPoints2) = {len(keyPoints2)}')\n",
    "print(f'type(keyPoints2[0]) = {type(keyPoints2[0])}')\n",
    "\n",
    "print(f'type(descriptors2) = {type(descriptors2)}')\n",
    "print(f'len(descriptors2) = {len(descriptors2)}')\n",
    "print(f'type(descriptors2[0]) = {type(descriptors2[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cv2::BFMatcher Class Reference](https://docs.opencv.org/3.4/d3/da1/classcv_1_1BFMatcher.html)\n",
    "* Brute-force descriptor matcher.\n",
    "\n",
    "\n",
    "[retval = cv.BFMatcher_create(\\[, normType\\[, crossCheck\\]\\])](https://docs.opencv.org/3.4/d3/da1/classcv_1_1BFMatcher.html#ac6418c6f87e0e12a88979ea57980c020)\n",
    "* `normType` - One of NORM_L1, NORM_L2, NORM_HAMMING, NORM_HAMMING2. L1 and L2 norms are preferable choices for SIFT and SURF descriptors, NORM_HAMMING should be used with ORB, BRISK and BRIEF, NORM_HAMMING2 should be used with ORB when WTA_K==3 or 4 (see ORB::ORB constructor description).\n",
    "* `crossCheck` - If it is false, this is will be default BFMatcher behaviour when it finds the k nearest neighbors for each query descriptor. If crossCheck==true, then the knnMatch() method with k=1 will only return pairs (i,j) such that for i-th query descriptor the j-th descriptor in the matcher's collection is the nearest and vice versa, i.e. the BFMatcher will only return consistent pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches. This is alternative to the ratio test, used by D. Lowe in SIFT paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the documentation (https://docs.opencv.org/3.4/d3/da1/classcv_1_1BFMatcher.html#abe0bb11749b30d97f60d6ade665617bd)\n",
    "# this constructor is obsolete:\n",
    "# bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "bf = cv2.BFMatcher_create(cv2.NORM_HAMMING, crossCheck=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[matches = cv2.DescriptorMatcher.match(queryDescriptors, trainDescriptors\\[, mask\\])](https://docs.opencv.org/3.4/db/d39/classcv_1_1DescriptorMatcher.html#a0f046f47b68ec7074391e1e85c750cba)\n",
    "* Finds the best match for each descriptor from a query set.\n",
    "* Parameters:\n",
    "   * `queryDescriptors` - Query set of descriptors.\n",
    "   * `trainDescriptors` - Train set of descriptors. This set is not added to the train descriptors collection stored in the class object.\n",
    "   * `matches` - Matches. If a query descriptor is masked out in mask , no match is added for this descriptor. So, matches size may be smaller than the query descriptors count.\n",
    "   * `mask` - Mask specifying permissible matches between an input query and train matrices of descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = bf.match(descriptors1, descriptors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(matches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cv::DMatch Class Reference](https://docs.opencv.org/3.4/d4/de0/classcv_1_1DMatch.html)\n",
    "\n",
    "Class for matching keypoint descriptors. Its public attributes are:\n",
    "   * queryIdx - query descriptor index\n",
    "   * trainIdx - train descriptor index\n",
    "   * imgIdx - train image index\n",
    "   * distance - distance between descriptors\n",
    "   \n",
    "Keypoints are not stored in DMatch, but in the other list. DMatch object only stores the indices of matched keypoints, their distance and the index of the image. You can get this indices to get the keypoints from the other list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_match = matches[0]\n",
    "single_match.distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sorted()](https://docs.python.org/3/library/functions.html#sorted) is Python built-in function that builds a new sorted list from an iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = sorted(matches, key=lambda x:x.distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cv2.drawMatches()](https://docs.opencv.org/master/d4/d5d/group__features2d__draw.html#gad8f463ccaf0dc6f61083abd8717c261a) helps us to draw the matches. It stacks two images horizontally and draw lines from first image to second image showing best matches\n",
    "\n",
    "```\n",
    "outImg = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches1to2, outImg[, matchColor[, singlePointColor[, matchesMask[, flags]]]])\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "   * `img1` - First source image.\n",
    "   * `keypoints1` - Keypoints from the first source image.\n",
    "   * `img2` - Second source image.\n",
    "   * `keypoints2` - Keypoints from the second source image.\n",
    "   * `matches1to2` - Matches from the first image to the second one, which means that keypoints1[i] has a corresponding point in keypoints2[matches[i]] .\n",
    "   * `outImg` - Output image. Its content depends on the flags value defining what is drawn in the output image. See possible flags bit values below.\n",
    "   * `matchColor` - Color of matches (lines and connected keypoints). If matchColor==Scalar::all(-1) , the color is generated randomly.\n",
    "   * `singlePointColor` - Color of single keypoints (circles), which means that keypoints do not have the matches. If singlePointColor==Scalar::all(-1) , the color is generated randomly.\n",
    "matchesMask\tMask determining which matches are drawn. If the mask is empty, all matches are drawn.\n",
    "   * `flags` - Flags setting drawing features. Possible flags bit values are defined by DrawMatchesFlags.\n",
    "This function draws matches of keypoints from two images in the output image. Match is a line connecting two keypoints (circles).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reeses_matches = cv2.drawMatches(reeses, keyPoints1, cereals, keyPoints2, matches[:25], None, flags=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(reeses_matches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MeanShift Tracking With OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F'cv2 version = {cv2.__version__}')\n",
    "# ouput: cv2 version = 4.5.0\n",
    "\n",
    "videoCaptureApi = cv2.CAP_ANY # autodetect default API\n",
    "\n",
    "# this works if conda-forge::opencv=4.5.0 is installed in the local environment\n",
    "cap = cv2.VideoCapture(\"/dev/video0\", videoCaptureApi)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"ERROR! Unable to open camera\")\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# setting up the initial tracking window\n",
    "# Previously we were using a corner detection to track 10 best corners detected in the 1st frame.\n",
    "# We now want to perform Face Tracking. We're gonna first do object detection in the first frame to grab a face location.\n",
    "# Then we're gonna detect a face as a bunch of pixels that we're gonna track. We'll then apply MeanShift tracking on that face.\n",
    "# We detect face one time at the beginning and then tell MeanShift algorithm to track that set of pixels. \n",
    "face_cascade = cv2.CascadeClassifier('../data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# returns a list of all NumPy arrays where it's detecting a face\n",
    "face_rects = face_cascade.detectMultiScale(frame)\n",
    "\n",
    "# We only want to track a single face so we'll grab the first face.\n",
    "\n",
    "# tuple of points is required form\n",
    "(face_x, face_y, w, h) = tuple(face_rects[0])\n",
    "\n",
    "# tracking window (a rectangle where face was detected in the first frame that we want to track)\n",
    "track_window = (face_x, face_y, w, h)\n",
    "\n",
    "# setup a ROI for tracking\n",
    "roi = frame[face_y:face_y+h, face_x:face_x+w]\n",
    "\n",
    "# using HSV color mapping\n",
    "hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], None, [180], [0, 180])\n",
    "\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# termination criteria (go for 10 iterations or at least one epsilon)\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret == True:\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
    "        ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n",
    "        x, y, w, h = track_window\n",
    "        img2 = cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 5)\n",
    "        cv2.imshow('img', img2)\n",
    "        \n",
    "        k = cv2.waitKey(1) &  0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are three screenshots of the video output for three different positions of my face.\n",
    "\n",
    "![](../img/MeanShift-1.png)\n",
    "![](../img/MeanShift-2.png)\n",
    "![](../img/MeanShift-3.png)\n",
    "\n",
    "Note that the red square keeps the same size even if I put my face closer to the camera. Its size is frozen to the size which matches the face detected in the first frame. It does not change later to match the different size of the face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CamShift Tracking with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F'cv2 version = {cv2.__version__}')\n",
    "# ouput: cv2 version = 4.5.0\n",
    "\n",
    "videoCaptureApi = cv2.CAP_ANY # autodetect default API\n",
    "\n",
    "# this works if conda-forge::opencv=4.5.0 is installed in the local environment\n",
    "cap = cv2.VideoCapture(\"/dev/video0\", videoCaptureApi)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"ERROR! Unable to open camera\")\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# setting up the initial tracking window\n",
    "# Previously we were using a corner detection to track 10 best corners detected in the 1st frame.\n",
    "# We now want to perform Face Tracking. We're gonna first do object detection in the first frame to grab a face location.\n",
    "# Then we're gonna detect a face as a bunch of pixels that we're gonna track. We'll then apply MeanShift tracking on that face.\n",
    "# We detect face one time at the beginning and then tell MeanShift algorithm to track that set of pixels. \n",
    "face_cascade = cv2.CascadeClassifier('../data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# returns a list of all NumPy arrays where it's detecting a face\n",
    "face_rects = face_cascade.detectMultiScale(frame)\n",
    "\n",
    "# We only want to track a single face so we'll grab the first face.\n",
    "\n",
    "# tuple of points is required form\n",
    "(face_x, face_y, w, h) = tuple(face_rects[0])\n",
    "\n",
    "# tracking window (a rectangle where face was detected in the first frame that we want to track)\n",
    "track_window = (face_x, face_y, w, h)\n",
    "\n",
    "# setup a ROI for tracking\n",
    "roi = frame[face_y:face_y+h, face_x:face_x+w]\n",
    "\n",
    "# using HSV color mapping\n",
    "hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], None, [180], [0, 180])\n",
    "\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# termination criteria (go for 10 iterations or at least one epsilon)\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret == True:\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
    "        \n",
    "        ret, track_window = cv2.CamShift(dst, track_window, term_crit)\n",
    "        pts = cv2.boxPoints(ret)\n",
    "        \n",
    "        # turn floating points to integers\n",
    "        pts = np.int0(pts)\n",
    "        img2 = cv2.polylines(frame, [pts], True, (0, 0, 255), 5)\n",
    "        \n",
    "        cv2.imshow('img', img2)\n",
    "        \n",
    "        k = cv2.waitKey(1) &  0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red square now gets resized to follow the size of the face.\n",
    "It's important to make sure that face is facing straight to camera when the first video frame is captured."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
